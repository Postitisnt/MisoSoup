{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to import required dependencies:\nnumpy: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.10 from \"/usr/local/bin/python3\"\n  * The NumPy version is: \"1.22.2\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: dlopen(/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/core/_multiarray_umath.cpython-310-darwin.so, 0x0002): tried: '/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/core/_multiarray_umath.cpython-310-darwin.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e'))\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/lorenzo/Desktop/SoupGroup/SoftwareDocumentation/SoftwareDocumentation.ipynb Cell 1'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lorenzo/Desktop/SoupGroup/SoftwareDocumentation/SoftwareDocumentation.ipynb#ch0000000?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/__init__.py:16\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         missing_dependencies\u001b[39m.\u001b[39mappend(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdependency\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[39mif\u001b[39;00m missing_dependencies:\n\u001b[0;32m---> 16\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m     17\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to import required dependencies:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(missing_dependencies)\n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     19\u001b[0m \u001b[39mdel\u001b[39;00m hard_dependencies, dependency, missing_dependencies\n\u001b[1;32m     21\u001b[0m \u001b[39m# numpy compat\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Unable to import required dependencies:\nnumpy: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.10 from \"/usr/local/bin/python3\"\n  * The NumPy version is: \"1.22.2\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: dlopen(/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/core/_multiarray_umath.cpython-310-darwin.so, 0x0002): tried: '/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/core/_multiarray_umath.cpython-310-darwin.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e'))\n"
     ]
    }
   ],
   "source": [
    "from pandas import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Introduction\n",
    "1. Data: characteristics, possible issues and resolutions\n",
    "2. Software organization: where is it possible to find what?\n",
    "3. Functioning\n",
    "4. Specification and things that do not work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em; line-height: 1.5em\">Our goal is to build a software that enables a possible user to query a relational and a graph database.</br>\n",
    "The aim of the project is to create a software that can handle the processing and the storing of data coming from CSV and JSON files into different databases, as well as providing the possibility to query these databases simultaneously by having in return specific lists of Python objects.</br>\n",
    "Firstly, we sterted by analysing the data we have been provided with in order to understand the different cases we had to handle and their characteristics. Then, the components used for the upload have been developed, taking in consideration the possibility of having to deal with only partial data. Here, some other issues contained into the starting data have been identified. As a consequence, we developed some additional functions aimed at correcting these problems. Finally, the query processors have been developed. Here we focused on the possibilty of creating a reliable piece of software that could be able to connect with different databases using different query languages, and to merge the information coming from the different origins in order to create complete python objects with respect to the classes defined with respect to our data model.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data characteristics, possible issues and resolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">We started by analyzing the exemplar JSON and CSV files that we have been provided with to test the software.</br>\n",
    "The CSV files are composed by the following columns:</p>\n",
    "\n",
    "<code><strong>id, title, type, publication_year, issue, volume, chapter, publication_venue, venue_type, publisher, event</strong></code>\n",
    "\n",
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">Each row defines a publication entity and the column \"publication_venue\" the relationship of that publication with its venue by mean of the latter's title.</br>\n",
    "When this column contains a value we know that the publications is contained in a venue, whose identifiers are specified in the JSON files along with additional data. In particular the JSON files are structured in 4 main keys:</p>\n",
    "\n",
    "<code><strong> authors, venues_id, references, publishers </strong></code>\n",
    "\n",
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">The first three sections contain additional information about authors, venues and citations which are connected to the related publication by means of the publications' unique identifiers used as sub-keys inside each of these 4 macro \"dictionaries\". The fourth key give further information about the publishers that can be connected to the inforamtion of our csv through the mediation of their crossref identifier, which is used as key inside the json.</p>\n",
    "\n",
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">The files have been analysed both manually and by means of Python in order to better understand both the quantitative and the qualitative characteristics fo the data. Here we started facing some \"problems\" that have been considered during the development of our software.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">The following cells show the quantitative characteristics of the data:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 503 entries, 0 to 502\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   id                 503 non-null    object \n",
      " 1   title              503 non-null    object \n",
      " 2   type               503 non-null    object \n",
      " 3   publication_year   503 non-null    int64  \n",
      " 4   issue              305 non-null    object \n",
      " 5   volume             393 non-null    object \n",
      " 6   chapter            93 non-null     float64\n",
      " 7   publication_venue  488 non-null    object \n",
      " 8   venue_type         488 non-null    object \n",
      " 9   publisher          488 non-null    object \n",
      " 10  event              1 non-null      object \n",
      "dtypes: float64(1), int64(1), object(9)\n",
      "memory usage: 43.4+ KB\n"
     ]
    }
   ],
   "source": [
    "graph_publication = read_csv(\"data/graph_publications.csv\")\n",
    "graph_publication.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>issue</th>\n",
       "      <th>volume</th>\n",
       "      <th>chapter</th>\n",
       "      <th>publication_venue</th>\n",
       "      <th>venue_type</th>\n",
       "      <th>publisher</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>503</td>\n",
       "      <td>503</td>\n",
       "      <td>503</td>\n",
       "      <td>503.000000</td>\n",
       "      <td>305</td>\n",
       "      <td>393</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>488</td>\n",
       "      <td>488</td>\n",
       "      <td>488</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>503</td>\n",
       "      <td>500</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38</td>\n",
       "      <td>116</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300</td>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>doi:10.1626637762/ueueeeeeeeeeeeeee</td>\n",
       "      <td>Supply Chain Coordination Based On Web Service</td>\n",
       "      <td>journal-article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Applied Sciences</td>\n",
       "      <td>journal</td>\n",
       "      <td>crossref:297</td>\n",
       "      <td>eruzione del Vesuvio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>409</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>408</td>\n",
       "      <td>174</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019.127237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.849462</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.780738</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.359536</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id  \\\n",
       "count                                   503   \n",
       "unique                                  503   \n",
       "top     doi:10.1626637762/ueueeeeeeeeeeeeee   \n",
       "freq                                      1   \n",
       "mean                                    NaN   \n",
       "std                                     NaN   \n",
       "min                                     NaN   \n",
       "25%                                     NaN   \n",
       "50%                                     NaN   \n",
       "75%                                     NaN   \n",
       "max                                     NaN   \n",
       "\n",
       "                                                 title             type  \\\n",
       "count                                              503              503   \n",
       "unique                                             500                3   \n",
       "top     Supply Chain Coordination Based On Web Service  journal-article   \n",
       "freq                                                 2              409   \n",
       "mean                                               NaN              NaN   \n",
       "std                                                NaN              NaN   \n",
       "min                                                NaN              NaN   \n",
       "25%                                                NaN              NaN   \n",
       "50%                                                NaN              NaN   \n",
       "75%                                                NaN              NaN   \n",
       "max                                                NaN              NaN   \n",
       "\n",
       "        publication_year issue volume    chapter publication_venue venue_type  \\\n",
       "count         503.000000   305    393  93.000000               488        488   \n",
       "unique               NaN    38    116        NaN               300          3   \n",
       "top                  NaN     1     11        NaN  Applied Sciences    journal   \n",
       "freq                 NaN    63     22        NaN                15        408   \n",
       "mean         2019.127237   NaN    NaN   0.849462               NaN        NaN   \n",
       "std             1.780738   NaN    NaN   0.359536               NaN        NaN   \n",
       "min          2011.000000   NaN    NaN   0.000000               NaN        NaN   \n",
       "25%          2018.000000   NaN    NaN   1.000000               NaN        NaN   \n",
       "50%          2019.000000   NaN    NaN   1.000000               NaN        NaN   \n",
       "75%          2021.000000   NaN    NaN   1.000000               NaN        NaN   \n",
       "max          2022.000000   NaN    NaN   1.000000               NaN        NaN   \n",
       "\n",
       "           publisher                 event  \n",
       "count            488                     1  \n",
       "unique            38                     1  \n",
       "top     crossref:297  eruzione del Vesuvio  \n",
       "freq             174                     1  \n",
       "mean             NaN                   NaN  \n",
       "std              NaN                   NaN  \n",
       "min              NaN                   NaN  \n",
       "25%              NaN                   NaN  \n",
       "50%              NaN                   NaN  \n",
       "75%              NaN                   NaN  \n",
       "max              NaN                   NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_publication.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   id                 500 non-null    object \n",
      " 1   title              500 non-null    object \n",
      " 2   type               500 non-null    object \n",
      " 3   publication_year   500 non-null    int64  \n",
      " 4   issue              347 non-null    object \n",
      " 5   volume             443 non-null    object \n",
      " 6   chapter            22 non-null     float64\n",
      " 7   publication_venue  498 non-null    object \n",
      " 8   venue_type         498 non-null    object \n",
      " 9   publisher          498 non-null    object \n",
      " 10  event              0 non-null      float64\n",
      "dtypes: float64(2), int64(1), object(8)\n",
      "memory usage: 43.1+ KB\n"
     ]
    }
   ],
   "source": [
    "relational_publication = read_csv(\"data/relational_publications.csv\")\n",
    "relational_publication.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>issue</th>\n",
       "      <th>volume</th>\n",
       "      <th>chapter</th>\n",
       "      <th>publication_venue</th>\n",
       "      <th>venue_type</th>\n",
       "      <th>publisher</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>347</td>\n",
       "      <td>443</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>498</td>\n",
       "      <td>498</td>\n",
       "      <td>498</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31</td>\n",
       "      <td>149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>295</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>doi:10.1162/qss_a_00023</td>\n",
       "      <td>Opencitations, An Infrastructure Organization ...</td>\n",
       "      <td>journal-article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Scientometrics</td>\n",
       "      <td>journal</td>\n",
       "      <td>crossref:297</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>478</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>478</td>\n",
       "      <td>145</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019.558000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.512353</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.294245</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id  \\\n",
       "count                       500   \n",
       "unique                      500   \n",
       "top     doi:10.1162/qss_a_00023   \n",
       "freq                          1   \n",
       "mean                        NaN   \n",
       "std                         NaN   \n",
       "min                         NaN   \n",
       "25%                         NaN   \n",
       "50%                         NaN   \n",
       "75%                         NaN   \n",
       "max                         NaN   \n",
       "\n",
       "                                                    title             type  \\\n",
       "count                                                 500              500   \n",
       "unique                                                500                2   \n",
       "top     Opencitations, An Infrastructure Organization ...  journal-article   \n",
       "freq                                                    1              478   \n",
       "mean                                                  NaN              NaN   \n",
       "std                                                   NaN              NaN   \n",
       "min                                                   NaN              NaN   \n",
       "25%                                                   NaN              NaN   \n",
       "50%                                                   NaN              NaN   \n",
       "75%                                                   NaN              NaN   \n",
       "max                                                   NaN              NaN   \n",
       "\n",
       "        publication_year issue volume    chapter publication_venue venue_type  \\\n",
       "count         500.000000   347    443  22.000000               498        498   \n",
       "unique               NaN    31    149        NaN               295          2   \n",
       "top                  NaN     1     11        NaN    Scientometrics    journal   \n",
       "freq                 NaN    69     29        NaN                50        478   \n",
       "mean         2019.558000   NaN    NaN   0.909091               NaN        NaN   \n",
       "std             1.512353   NaN    NaN   0.294245               NaN        NaN   \n",
       "min          2014.000000   NaN    NaN   0.000000               NaN        NaN   \n",
       "25%          2018.000000   NaN    NaN   1.000000               NaN        NaN   \n",
       "50%          2020.000000   NaN    NaN   1.000000               NaN        NaN   \n",
       "75%          2021.000000   NaN    NaN   1.000000               NaN        NaN   \n",
       "max          2021.000000   NaN    NaN   1.000000               NaN        NaN   \n",
       "\n",
       "           publisher  event  \n",
       "count            498    0.0  \n",
       "unique            32    NaN  \n",
       "top     crossref:297    NaN  \n",
       "freq             145    NaN  \n",
       "mean             NaN    NaN  \n",
       "std              NaN    NaN  \n",
       "min              NaN    NaN  \n",
       "25%              NaN    NaN  \n",
       "50%              NaN    NaN  \n",
       "75%              NaN    NaN  \n",
       "max              NaN    NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relational_publication.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 540 entries, doi:10.1162/qss_a_00023 to crossref:301\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   authors     508 non-null    object\n",
      " 1   venues_id   498 non-null    object\n",
      " 2   references  500 non-null    object\n",
      " 3   publishers  32 non-null     object\n",
      "dtypes: object(4)\n",
      "memory usage: 21.1+ KB\n"
     ]
    }
   ],
   "source": [
    "relational_other_data = read_json(\"data/relational_other_data.json\")\n",
    "relational_other_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>venues_id</th>\n",
       "      <th>references</th>\n",
       "      <th>publishers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>508</td>\n",
       "      <td>498</td>\n",
       "      <td>500</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>486</td>\n",
       "      <td>297</td>\n",
       "      <td>99</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>[{'family': 'Leydesdorff', 'given': 'Loet', 'o...</td>\n",
       "      <td>[issn:0138-9130, issn:1588-2861]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'id': 'crossref:6228', 'name': 'Codon Publica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>366</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  authors  \\\n",
       "count                                                 508   \n",
       "unique                                                486   \n",
       "top     [{'family': 'Leydesdorff', 'given': 'Loet', 'o...   \n",
       "freq                                                    4   \n",
       "\n",
       "                               venues_id references  \\\n",
       "count                                498        500   \n",
       "unique                               297         99   \n",
       "top     [issn:0138-9130, issn:1588-2861]         []   \n",
       "freq                                  50        366   \n",
       "\n",
       "                                               publishers  \n",
       "count                                                  32  \n",
       "unique                                                 32  \n",
       "top     {'id': 'crossref:6228', 'name': 'Codon Publica...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relational_other_data.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 568 entries, doi:dioMadonna to crossref:4443\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   authors     530 non-null    object\n",
      " 1   venues_id   490 non-null    object\n",
      " 2   references  501 non-null    object\n",
      " 3   publishers  38 non-null     object\n",
      "dtypes: object(4)\n",
      "memory usage: 22.2+ KB\n"
     ]
    }
   ],
   "source": [
    "graph_other_data = read_json(\"./data/graph_other_data.json\")\n",
    "graph_other_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>venues_id</th>\n",
       "      <th>references</th>\n",
       "      <th>publishers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>530</td>\n",
       "      <td>490</td>\n",
       "      <td>501</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>496</td>\n",
       "      <td>313</td>\n",
       "      <td>101</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>[{'family': 'Pal', 'given': 'Kamalendu', 'orci...</td>\n",
       "      <td>[issn:2076-3417]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'id': 'crossref:9011001109', 'name': 'Nacc'a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>332</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  authors         venues_id  \\\n",
       "count                                                 530               490   \n",
       "unique                                                496               313   \n",
       "top     [{'family': 'Pal', 'given': 'Kamalendu', 'orci...  [issn:2076-3417]   \n",
       "freq                                                    4                15   \n",
       "\n",
       "       references                                         publishers  \n",
       "count         501                                                 38  \n",
       "unique        101                                                 38  \n",
       "top            []  {'id': 'crossref:9011001109', 'name': 'Nacc'a ...  \n",
       "freq          332                                                  1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_other_data.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">The main things we have noticed by analysing the quantitaive aspects were:</p>\n",
    "<ol style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "    <li>The number of unique publications in the JSON and CSV files could be different, so even if they match, they do it only partially.</li>\n",
    "    <li>The venues ids are multivalues and they can be defined both with a single or with a multiple id inside the JSON files.</li>\n",
    "</ol>\n",
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">Other issues came from the structure of the files. In particular, in the CSV files defining the publication's entities, there are data relative to the venues, for ex. the title of the venues, that cannot be connected to a venue without an internal identifier, the only reliable source to identify and define unique venues. Information about the identifiers of the venues are only present in the JSON files, so those information cannot be added and the internal id of the venues cannot be created until a JSON that matched the CSV data is uploaded.</p>\n",
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">From the qualitative analysis, carried out by looking directly at the data contained in the exemplar files, we noticed that there are data related to the same identifiers that are reported in different ways. For example:</p>\n",
    "<ol style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "    <li>The given name of a person identified with the same identifier</li>\n",
    "    <li>The same venue, connected with different publications, has always the same title but in some occurrences the \"&\" character is written as the XML entity reference \"&Amp;\".</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">The main solutions we have identified in order to manage this issues are:</p>\n",
    "<ol style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "    <li>An external JSON file is automatically created by the execution of the upload of the data. Here the data of the venues are stored while waiting to be connected with their specific identifier. In particular, we developed a small function to name the file automatically depending on the endpoint or the path of the database. In this way each database that will be created, will have its own specifc JSON file from which data can be imported or even queried.</li>\n",
    "    <li>When defining the venues by means of their ids, we have used a double iteration that checks first whether one of the ids is already assigned to a venue, and then iterate over the ids another time to bound them to the right internal id.</li>\n",
    "    <li>A specific function handle the XML entities contained in the CSV files by replacing the most common entities with their respective charater each time the software reads a CSV file.</li>\n",
    "    <li>Every time an already existing entity is encoutered, we keep the last version of the data, so if the user made a typing error or wants to update its data can easily do that by uploading another file. In the triplestore data processor this process is made by checking if the data related to it are the same and, if there is a new data, depending on the arity defined in the data model, we decide whether to add a new value or to keep the last added, as if an update occurred. Naturally we do that only for \"static\" data, so that the relations between different entities are not touched.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">Moreover, since the data types are interpreted every time they are uploaded to and downloaded from the databases, once in Python, we decided to convert them into string in order to have more possiblities for the manipulation. Accordingly to the specifications of the various predicates, they are converted into the right data type before the upload.\n",
    "Naturally, once the databases are queried via generic query processor, the python object returned to the user contain data compliant with the data model.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Software organisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">We organized the software dividing it in different pieces of code contained in different files that can be easily managed. In order to describe the aim and how each piece of software was developed we will devide the following section in subsections that have the same division and titles of the different files. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 classesDataModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">The file contains the definition of the classes of the main data model, so the ones related to the entities that have to be returned to the user as Python object.</br>\n",
    "It is imported in all the other files that make use of the classes here defined.</p><p>\n",
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">Each class has also its specific methods according to the data model. However, we decided to add a new method called <code>addCitedPublication()</code> to the class <code>Publication</code>, that will add each citation to the attribute cites. This has been done in order to avoid that the code could run in an infinite loop, which happens by adding the citations in the generic processor by means of the parameter of that class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"datamodel.png\" width=80%/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 additionalClasses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">It contains the definition of the classes: <code>RelationalProcessor</code>, <code>QueryProcessor</code> and <code>TripleStoreProcessor</code>. We decided to put these three classes here because they are direct subclasses of the class <code>object</code>.</br>\n",
    "This file is naturally imported in all the other files that make use of the classes here defined.</P>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/uml2.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 URIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "This file contains only the variables to which the URIs used by the triplestoreDatProcessor are assigned, and it is imported by every other file that uses them. It includes URIs that refer both to classes and properties.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 relationalDataProcessor & triplestoreDataProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "In the two files, the classes <code>relationalDataProcessor</code> and <code>triplestoreDataProcessor</code> are defined.\n",
    "They both have a simple structure that checks what kind of files have been already uploaded into the databases and, depending on the specific case, recalls an external function aimed at managing the upload of either a CSV or a JSON file.</br>\n",
    "These functions, as well as the other functions used by all the processors are explained in another section (see 2.7 relationalUploadFunction & triplestoreFunctions).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "We use the following functions in both the data processors in order to retrieve information from the csv and the json file.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\bianc\\OneDrive\\Desktop\\documentazioni\\DocumentazioneComplessiva.ipynb Cell 35'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/bianc/OneDrive/Desktop/documentazioni/DocumentazioneComplessiva.ipynb#ch0000055?line=2'>3</a>\u001b[0m path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m#put there the path of a csv to see how it works\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/bianc/OneDrive/Desktop/documentazioni/DocumentazioneComplessiva.ipynb#ch0000055?line=4'>5</a>\u001b[0m \u001b[39m#READ THE CSV FILE\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/bianc/OneDrive/Desktop/documentazioni/DocumentazioneComplessiva.ipynb#ch0000055?line=5'>6</a>\u001b[0m publications \u001b[39m=\u001b[39m read_csv(path,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/bianc/OneDrive/Desktop/documentazioni/DocumentazioneComplessiva.ipynb#ch0000055?line=6'>7</a>\u001b[0m keep_default_na\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/bianc/OneDrive/Desktop/documentazioni/DocumentazioneComplessiva.ipynb#ch0000055?line=7'>8</a>\u001b[0m dtype\u001b[39m=\u001b[39;49m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/bianc/OneDrive/Desktop/documentazioni/DocumentazioneComplessiva.ipynb#ch0000055?line=8'>9</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mstring\u001b[39;49m\u001b[39m\"\u001b[39;49m,               \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bianc/OneDrive/Desktop/documentazioni/DocumentazioneComplessiva.ipynb#ch0000055?line=9'>10</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mtitle\u001b[39;49m\u001b[39m\"\u001b[39;49m : \u001b[39m\"\u001b[39;49m\u001b[39mstring\u001b[39;49m\u001b[39m\"\u001b[39;49m,            \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bianc/OneDrive/Desktop/documentazioni/DocumentazioneComplessiva.ipynb#ch0000055?line=10'>11</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mtype\u001b[39;49m\u001b[39m\"\u001b[39;49m : \u001b[39m\"\u001b[39;49m\u001b[39mstring\u001b[39;49m\u001b[39m\"\u001b[39;49m ,             \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bianc/OneDrive/Desktop/documentazioni/DocumentazioneComplessiva.ipynb#ch0000055?line=11'>12</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mpublication_year\u001b[39;49m\u001b[39m\"\u001b[39;49m : \u001b[39m\"\u001b[39;49m\u001b[39mstring\u001b[39;49m\u001b[39m\"\u001b[39;49m,   \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bianc/OneDrive/Desktop/documentazioni/DocumentazioneComplessiva.ipynb#ch0000055?line=12'>13</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39missue\u001b[39;49m\u001b[39m\"\u001b[39;49m : \u001b[39m\"\u001b[39;49m\u001b[39mstring\u001b[39;49m\u001b[39m\"\u001b[39;49m,              \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bianc/OneDrive/Desktop/documentazioni/DocumentazioneComplessiva.ipynb#ch0000055?line=13'>14</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mvolume\u001b[39;49m\u001b[39m\"\u001b[39;49m :\u001b[39m\"\u001b[39;49m\u001b[39mstring\u001b[39;49m\u001b[39m\"\u001b[39;49m,              \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bianc/OneDrive/Desktop/documentazioni/DocumentazioneComplessiva.ipynb#ch0000055?line=14'>15</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mchapter\u001b[39;49m\u001b[39m\"\u001b[39;49m :\u001b[39m\"\u001b[39;49m\u001b[39mstring\u001b[39;49m\u001b[39m\"\u001b[39;49m ,         \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bianc/OneDrive/Desktop/documentazioni/DocumentazioneComplessiva.ipynb#ch0000055?line=15'>16</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mpublication_venue\u001b[39;49m\u001b[39m\"\u001b[39;49m : \u001b[39m\"\u001b[39;49m\u001b[39mstring\u001b[39;49m\u001b[39m\"\u001b[39;49m ,  \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bianc/OneDrive/Desktop/documentazioni/DocumentazioneComplessiva.ipynb#ch0000055?line=16'>17</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mvenue_type\u001b[39;49m\u001b[39m\"\u001b[39;49m :\u001b[39m\"\u001b[39;49m\u001b[39mstring\u001b[39;49m\u001b[39m\"\u001b[39;49m,           \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bianc/OneDrive/Desktop/documentazioni/DocumentazioneComplessiva.ipynb#ch0000055?line=17'>18</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mpublisher\u001b[39;49m\u001b[39m\"\u001b[39;49m :\u001b[39m\"\u001b[39;49m\u001b[39mstring\u001b[39;49m\u001b[39m\"\u001b[39;49m ,          \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bianc/OneDrive/Desktop/documentazioni/DocumentazioneComplessiva.ipynb#ch0000055?line=18'>19</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mevent\u001b[39;49m\u001b[39m\"\u001b[39;49m :\u001b[39m\"\u001b[39;49m\u001b[39mstring\u001b[39;49m\u001b[39m\"\u001b[39;49m   \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bianc/OneDrive/Desktop/documentazioni/DocumentazioneComplessiva.ipynb#ch0000055?line=19'>20</a>\u001b[0m })\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bianc/OneDrive/Desktop/documentazioni/DocumentazioneComplessiva.ipynb#ch0000055?line=21'>22</a>\u001b[0m publications\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m     f,\n\u001b[0;32m   1219\u001b[0m     mode,\n\u001b[0;32m   1220\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1221\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1222\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1223\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1224\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1225\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1226\u001b[0m )\n\u001b[0;32m   1227\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    790\u001b[0m             handle,\n\u001b[0;32m    791\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    792\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    793\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    794\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    795\u001b[0m         )\n\u001b[0;32m    796\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "path = \"\" #put there the path of a csv to see how it works\n",
    "\n",
    "#READ THE CSV FILE\n",
    "publications = read_csv(path,\n",
    "keep_default_na=False,\n",
    "dtype= {\n",
    "    \"id\": \"string\",               \n",
    "    \"title\" : \"string\",            \n",
    "    \"type\" : \"string\" ,             \n",
    "    \"publication_year\" : \"string\",   \n",
    "    \"issue\" : \"string\",              \n",
    "    \"volume\" :\"string\",              \n",
    "    \"chapter\" :\"string\" ,         \n",
    "    \"publication_venue\" : \"string\" ,  \n",
    "    \"venue_type\" :\"string\",           \n",
    "    \"publisher\" :\"string\" ,          \n",
    "    \"event\" :\"string\"   \n",
    "})\n",
    "\n",
    "publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import load\n",
    "\n",
    "nameOfJson = \"\" #put there the path of a csv to see how it works\n",
    "\n",
    "#READ THE JSON FILE \n",
    "with open(nameOfJson, \"r\", encoding=\"utf-8\") as jsonVenue:\n",
    "    venueData = load(jsonVenue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 relationalQueryProcessor & triplestoreQueryProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "The files contain, respectively, the definition of the classes <code>relationalQueryProcessor</code> and <code>triplestoreQueryProcessor</code>.</br>\n",
    "In the <code>relationalQueryProcessor</code>, the methods execute SQL queries by using the python library <code>sqlite3</code> with the aim of retrieving the desired data from the relational database. Here, when necessary, data are also retrieved from the external JSON containing the data related to the venues with missing data, i.e. their ids. Each method of the class returns a pandas DataFrame with the information necessary in order to build the python objects requested.</br>\n",
    "In the <code>triplestoreQueryProcessor</code>, the methods execute SPARQL queries in order to return the desired data, while the handling of the pandas DataFrame, as well as the management of the data coming from the external JSON, are demanded to specific functions developed for these purposes.</p>\n",
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "Some additional methods have been defined for both classes to handle the additional requests coming from the generic query processor when the retrieved data are not enough to create the complete Python object that has to be returned to the user. The methods are:\n",
    "</p>\n",
    "<ol style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "    <li><code>getPublicationByDoi()</code>: it returns a dataframe with the information about a specific publication that has as identifier the input doi.</li>\n",
    "    <li><code>getAuthorById()</code>: it returns a dataframe with the information about all the authors of a specific publicatioin that has as identifier the input doi.</li>\n",
    "    <li><code>getPublisherById()</code>: it returns a dataframe with the information about the publisher of a publication that has as identifier the input doi.</li>\n",
    "    <li><code>getVenuesByDoi()</code>: it returns a dataframe with the all the ids of a venue that contains the publication that has as identifier the input doi.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 genericQueryProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "The class <code>genericQueryProcessor</code> contains only the methods shown in the data model, which call the corresponding methods of the relational and the triplestore query processors. These methods can call also some external functions that we defined with the purpose of addressing recurring cases that could be afford with the same code. Each method returns a list of python model-compliant object that are delivered to the user, ready to be resused with their own methods, defined in the file \"classesDataModel\" (see 2.1).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 relationalUploadFunctions & triplestoreFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "In the <code>relationalUploadFunctions</code>, can be found the functions called by the <code>relationalDataProcessor</code>.\n",
    "In particular:\n",
    "</p>\n",
    "<ol style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "    <li><code>replace_entity_reference()</code> is called to replace the occurrences of the most common entity references in the \"title\" and \"publication_venue\" column when reading a CSV file with pandas. It takes in input the dataframe coming from the CSV file and return the same dataframe with the correct replacements.</li>\n",
    "    <li><code>upload_csv()</code> is called in the case in which a CSV has to be uploaded in the relational database. It takes in input the CSV file and the database's path and manages both the first upload and the update of the tables into the database.</li>\n",
    "    <li><code>upload_csv()</code> is called when a JSON has to be uploaded in the relational database. It takes in input both the JSON file and the database's path and manages both the first upload and the update of the tables into the database.</li>\n",
    "</ol>\n",
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "In the second file, more functions used by the <code>triplestoreDataProcessor</code> and the <code>triplestoreQueryProcessor</code>, as well as some queries that are reused by the function themselves, are defined.\n",
    "Aside from the queries, the functions are:\n",
    "<ol style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "    <li><code>csv_exists()</code> and <code>json_exists()</code> take in input the Blazegraph database's endpoint and return a boolean value. They are used by the data processor to check whether a CSV or a JSON file has been already uploaded into the database.</li>\n",
    "    <li><code>replace_entity_reference()</code> is called to replace the occurrences of the most common entity references in the \"title\" and \"publication_venue\" column when reading a CSV file with pandas in the data processor. It takes in input the dataframe coming from the CSV file and return the same dataframe with the correct replacements.</li>\n",
    "    <li><code>fix_df_columns_encoding()</code> is called by the triplestore query processor to fix the encoding of the literals coming from the Blazegraph database in order to return a data frame with the values encoded in utf-8 to the generic query processor. It takes in input a dataframe and three boolean values that are used to understand whether the dataframe contains columns related to publications, to venues and to events, and returns the same dataframe with the correct encoding.</li>\n",
    "    <li><code>replace_dot_zeros()</code> is called by the triplestore query processor to fix the \".0\" that appear in some columns when Python interpret the data types in the wrong way. It takes in input a dataframe and a boolean value used to understand when it has to deal with the column related to \"chapter number\", and returns the same dataframe with the correct replacements.</li>\n",
    "    <li><code>upload_in_store()</code> is called by the data processor to upload the triples contained into the local graph to the Blazegraph database. It takes in input the instance of the class \"SPARQLUpdateStore\", the endpoint of the Blazegraph database, and the local graph containing the triples to upload.</li>\n",
    "    <li><code>update_store()</code> is called by the data processor to both upload the triples contained into the local graph to the Blazegraph database and to delete the ones that contain data that have found to be up to be dated. It takes in input the same parameters as the previous function plus the other local graph containing the triples to delete.</li>\n",
    "    <li><code>read_csv_file()</code> <code>read_json_file()</code> are just recalled when it is necessary to read one CSV or JSON file in pandas. They take in input the path of the file and return a pandas dataframe (the first) and a dictionary (the second).</li>\n",
    "    <li><code>name_additional_data_file()</code> is called by the data processor to name the file and the path of the external JSON file depending on the endpoint URL of the database. It takes in input the Blazegraph endpoint and creates a new JSON file.</li>\n",
    "    <li><code>upload_additional_data()</code> is called by the data processor to store the data related to the non-defined venues into the exteral JSON file. It takes in input the path of the JSON file that stores the additional data related to the venues and the dictionary containing the data to insert in the JSON itself.</li>\n",
    "    <li><code>additional_data_dataframe()</code> is called by the triplestore query processor to create a dataframe from the JSON containing the information about the venues. This dataframe is created to be concatenated to the one built from the query ran by the called method. It takes in input the list containing the headers of the dataframe generated by the query, the path of the JSON from which the new dataframe has to be created, the list cof internal ids created from one of the columns of the query dataframe (depending on the query), the blazegraph endpoint, the key of the dictionary used to make the checks useful to create the dataframe from the JSON (depending on the query), the list of publisher internal ids or an empty string (depending on the query), the crossref identifier the user is searching for or an empty string (depending on the query), and the partial string the user used to retrieve a venue by its event or an empty string (depending on the query).</li>\n",
    "    <li><code>retrieve_doi()</code> and <code>retrieve_crossref()</code> are two recursive functions called inside the previous function and are used to convert the internal identifiers of the publications and the publishers into doi and crossref identifiers. They take in input the Blazegraph endpoint, the list of internal ids that have to be converted and an empty list, and return the list of identifiers that will be used to create the specific column of the dataframe in the previous function.</li>\n",
    "    <li><code>csv_upload()</code> and <code>json_upload()</code> are called by the data processor to upload the data contained, respectively, into a CSV file or a JSON file to the Blazegraph database when the latter is empty.</li>\n",
    "    <li><code>csv_to_csv()</code> and <code>csv_to_json()</code> are called by the data processor to upload the data contained into a CSV file to the Blazegraph database when it contains, respectively, either data coming from the CSV or data coming from a JSON file.</li>\n",
    "    <li><code>json_to_csv()</code> and <code>json_to_json()</code> are called by the data processor to upload the data contained into a JSON file to the Blazegraph database when it contains, respectively, either data coming from the CSV or data coming from a JSON file.</li> is called by the data processor to name of the file and the path of the external JSON file depending on the endpoint URL of the database.</li>    \n",
    "</ol>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Functioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Relational Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "It is the parent class of both the <code>RelationalDataProcessor()</code> and the <code>RelationalQueryProcessor</code> and has the attribute <code>dbPath</code> used to get and change the path of the database through two functions: \n",
    "<ul>\n",
    "<li><code>getDbPath()</code>: it returns the path of the database.</li>\n",
    "<li><code>setDbPath()</code>: it allows to changes the path of the database.</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Relational Data Processor - Upload Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "The class <code>RelationalDataProcessor()</code> has only the method <code>uploadData()</code> that takes in input the path of the file the user wants to update that can be both a csv or a json. The function checks if the file is empty, otherwise it checks weather it has the <code>.csv</code> extension or the <code>.json</code> extension. In the first case it will run the external function <code>upload_csv()</code>, otherwise it will run the external function <code>upload_json()</code>.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 <code>upload_csv()</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "With information from a CSV we are able to populate the tables JournalArticles, BookChapters, PorceedingsPapers, PublicationsIds, PublishersIds. </br>\n",
    "We also have information about the venue name and the venue type but, since we have no information about the venue identifier, we can't complete our venues' tables yet. As a matter of fact, we can't be sure that two venues with the same name are actually the same venue, we will only be able to uniquely identify them, when receiving identifiers information from a JSON.\n",
    "</br>\n",
    "However, for practicality and in order to handle better SQL queries, that raise error when asking for a table that is not in the database, in the class <code>RelationalQueryProcessor</code> of our software, we opted for the creation of <u>all the tables with all the headers, leaving empty values where there are no information for to add yet</u>. These data will be added later when a JSON is uploaded.\n",
    "</p>\n",
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "Those information will not be updated in the following cases:\n",
    "<ol style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "<li>if only a CSV is passed as input of the uploadData function with the current database path.</li>\n",
    "<li>Both CSV and JSON files are passed as input in the current databse but their information do not match with each other (ex. the publication identifiers in the JSON are different from those present in the CSV)</li>\n",
    "<li>The arrity of the attribute/relation can be 0, meaning that the information could not be present, but the entity is still compliant with our data model.</li>\n",
    "</ol>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "In order to know if the tables must be uploaded for the first time or wether they are already in the database we use some try/except blocks: in the try block we ask for the tables, if there aren't the code will procede with the except block that will create those tables. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "The code is structured as follows: \n",
    "<ul style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "<li>First we read the CSV file and we assign the type string to all the columns</li> \n",
    "<li>The first try/except code, we check whether the tables that can be created both from a JSON file and a CSV file, i.e. PublicationsIds and PublishersIds, are already in the database. If there are not, we create the tables for the first time.</li>\n",
    "<li>Outside of the try - except, we create the tables of the publications by querying the whole dataframe created from the csv, asking for rows of a specific type of publication and merging the new dataframe with the PublicationsIds in order to add the internal id and we keep for each table, i.e. JournalArticles, BookChapters and ProceedingsPapers, only the columns that are compliant to the data model. \n",
    "<li>In another try/except clause we update the publications' tables if they already exists we the new data and we add (or update in case it is not the first file).\n",
    "</li>\n",
    "<li>In the same try/except clause we check wether there are already the venues' tables in order to update them with the new data. The data about the venues that do not match a venue id will be loaded in an external JSON (see 2.5). If the tables of the veneus are not already in the database all the data will be directly loaded in the external JSON</li>\n",
    "<li>If none of those tables was already in the database the one that can be created only by means of a JSON file, e.g. AuthorsIds, will be created as empty dataframe with a header compliant to the data model.</li>\n",
    "<li>Finally we upload all the tables in the database.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 <code>upload_json()</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "For the <code>upload_json()</code> function, the overall logic we followed is the same:\n",
    "<ul style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "<li>First we see if the PublicationsIds and the PublishersIds must be updated or it is the first time we upload them. We ask also the VenuesIds table and the Publishers tables, since we can immediately create them and we know that all the tables are created with the first upload.</li>\n",
    "<li>If it is the first file uploaded in the database we are able to create and populate the following tables: PublicationsIds, Publishers, PublishersIds, VenuesIds, PublicationsVenues, Citations, Authors, AuthorsIds. All the other tables will be created as empty dataframes with headers compliant to the data model.</li>\n",
    "<li>If those tables are already present in the database, i.e. the SQL's queries succeed, all the tables mentioned above are updated.</li>\n",
    "<li>If there are already tables inside the database, through PublicationsVenues table, we associate the venue internal id as a foreign key to ours venues' tables, i.e. Journals, Books and Proceedings. We also download the venues information coming from the CSV that were previously stored in our additional JSON file, and see if they can be associated through the doi to the venues' tables.</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Triplestore Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "It is the parent class of both the <code>TriplestoreDataProcessor()</code> and the <code>TriplestoreQueryProcessor</code> and has the attribute <code>endPointUrl</code> used to get and change the enpoint of the database through two functions: \n",
    "<ul>\n",
    "<li><code>getEndPointUrl()</code>: it returns the path of the database.</li>\n",
    "<li><code>setEndPointUrl()</code>: it allows to changes the path of the database.</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Triplestore Data Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "The <code>uploadData()</code> method of the <code>TriplestoreDataProcessor()</code> has the same aim of the one of the <code>RelationalDataProcessor()</code>. \n",
    "The function checks at first the existence of a string type path. If this basic condition is fulfilled, the methods go on with additional checks on the kind of path that is defined as input.</br>\n",
    "These additional checks are in particular two: the first one is useful for CSV formats, while the second one for JSON formats. Seen the fact that the upload is thought only for JSON and CSV, if the path of the file is not of one of these two different types the upload method fails, returning False to the user.\n",
    "</br>\n",
    "Once inside one of these two conditions, the methods checks if inside our database we have yet data coming from a CSV, a JSON, or coming by both. According to the file path and to the previous uploads, i.e. to the fact that there can be already data coming from a JSON or from a CSV file, it executes of the six functions according to the scenarios explained below:\n",
    "</p>\n",
    "\n",
    "<ol style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "<li><code>csv_upload()</code>: it uploads data coming from a CSV file when the database is empty</li>\n",
    "<li><code>json_upload()</code>: it uploads data coming from a JSON file when the database is empty</li>\n",
    "<li><code>csv_to_csv()</code>: it uploads data coming from a CSV when another CSV was already uploaded in the database</li>\n",
    "<li><code>json_to_csv()</code>: it uploads data coming from a JSON when a CSV was already uploaded in the database</li>\n",
    "<li><code>json_to_json()</code>: it uploads data coming from a JSON when another JSON was already uploaded in the database</li>\n",
    "<li><code>csv_to_json()</code>: it uploads data coming from a CSV when a JSON was already uploaded in the database</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Query Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "It is the parent class of both the <code>RelationalQueryProcessor()</code> and the <code>TriplestoreQueryProcessor()</code> and it has no attributes or relations.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Relational Query Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "The structure of the <code>RelationalQueryProcessor()</code> has been defined above (see 2.5). \n",
    "We managed the queries by means of the library <code>sqlite3</code>. The main syntax of SQL that we used is the follows: \n",
    "<ul style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "<li>SELECT <\\name_of_the_column>: to select the columns we wanted in our dataframe.</li>\n",
    "<li>LEFT JOIN <\\table1> ON <\\table2>.<\\name_of_the_column> == <\\table1>.<\\name_of_the_column>: to join the tables on the columns that have the same values</li>\n",
    "<li>UNION ALL: to join the results of queries</li>\n",
    "<li>WHERE <\\name_of_the_column> = '\"\"\"+<\\name_of_the_python_variable>+\"\"\"': to set the condition according to the variable in input in the queries</li>\n",
    "</ul>\n",
    "</p>\n",
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "We used also an external function <code>getVenues()</code> in order to retrieve a dataframe with all the data of the venues that still did not match any venue id, i.e. that have not be added to the database since they are not compliant to the data model, but that still can have useful information for our query. The functions takes in input the dataframe of the method that calls it in order to return a dataframe with the same headers that can be easily add to the output dataframe.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Triplestore Query Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "The structure of the <code>RelationalQueryProcessor()</code> has been defined above (see 2.5). \n",
    "We managed the queries by means of the library <code>sparql_dataframe</code>.\n",
    "<ul style=\"font-size: 1.2em; line-height: 1.5em;\">>\n",
    "<li>SELECT ?variable: it sets only the values that must be returned by the query</li>\n",
    "<li>WHERE {<\\condition>}: inside the brackets there is a triple that is the condition that must be fullfilled</li>\n",
    "<li>OPTIONAL {<\\condition>}: inside the brackets there is a triple that is a condition that it is not necessary in order to return the values </li>\n",
    "<li>FILTER: it filters the results according to the value of a variable</li>\n",
    "<li>FILTER NOT EXIST {}: it filters the elements that do not have the triple inside the brackets</li>\n",
    "<li>UNION: to join the results of the queries</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Generic Query Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "The structure and the aims of the class have already been described above (see 2.6).\n",
    "It recalls some external functions that we added in order to run common patterns: \n",
    "<ul style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "<li><code>getPublicationObject()</code>: it is a recursive function that takes in input a doi, a dictionary, a dataframe and the list of the query processors objects. It has the aim of create a publication object and it is recursive because it creates also the python object of the publications it cites (and of the publications cited by its citation until it matches a base case). It returns a tuple with a dictionary with all the objects created and the python object of the publication we wanted to create. If it is not possible to create the python object (this happens when necessary information are not complete) it returns the object as an empty string in order to return also this information to the function that calls it.</li>\n",
    "<li><code>searchForCitationObject()</code>: it takes in input two dictionaries and the list of the query processors objects and it returns a dictionary of publications and a dictionary with the publications without type.</li>\n",
    "<li><code>addPublicationToDict()</code>: it takes in input a row, a dictionary of publications and the dictionaries of publications without type and it returns the two updated dictionaries</li>\n",
    "<li><code>updatePublicationWithType()</code>: it takes in input a row, a dictionary of publications and the dictionaries of publications without type and it returns the two dictionaries with the update of the publications that now have a type.</li>\n",
    "<li></code>getMostCitedInDataframe()</code>: it is a recursive function that takes in input the dictionary of the counts, the dictionary of the publications objects and the list of the query processors objects and returns the object with the most number of citations.</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em; line-height: 1.5em;\">\n",
    "The software returns only objects compliant with the database. It means that an object of class JpurnalArticle citing two other publications, will contain, in the attribute \"cites\" only compliant objects. If one of these publication objects cannot be created, the information about that specific citation will be lost.\n",
    "In other words, if the sum of the data contained into the different databases is still incomplete in some way, the object that the software returns will be always compliant to the data model, therefore containing compliant but partial information. </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
